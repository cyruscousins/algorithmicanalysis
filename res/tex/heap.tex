Perhaps the quintisential priority queue data structure, heaps are popular for their simplicity and speed.  Asymptotically superior data structures exist, but heaps are still widely used in practice due to their low overhead, small constant cost factors, and practicality.  An excellent introduction to the heap data structure can be found \href{http://www.cprogramming.com/tutorial/computersciencetheory/heap.html}{here}, and a visualization from the University of San Francisco can be found \href{http://www.cs.usfca.edu/~galles/visualization/Heap.html}{here}.

Discussion of interesting exact cost analyses follow.

\begin{enumerate}[a):]

\item \texttt{construct}: Using the linear time heap construction algorithm, analysis through convergent series yields the bound of $2^{\ceil{\log_2 n}}$.

\item $E[$\texttt{decrease\_key}$]$: This expectation holds when the position of the key to be decreased is assumed to be random and the value to be decreased to is minimal for the heap\footnote{This decision was made because a more complicated assumption, which could yield a lower bound, would involve making assumptions about the values in the heap that are likely to be violated frequently.}, and $n$ is given by $2^x - 1$ for some $x \in \mathbb{Z}$.  For other $n$, this is still a valid upper bound.  Now, setting the probability of selecting each element of the heap to $\frac{1}{n}$, $E[$\texttt{decrease\_key}$] = \sum_{i = 0}^n \frac{1}{n} \ceil{\log_2 i} = \frac{1}{n} \sum_{i = 0}^n \ceil{\log_2 n}$.

\end{enumerate}
